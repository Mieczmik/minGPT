{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23370501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6543f",
   "metadata": {},
   "source": [
    "# Embedding - czyli zamienianie słów na wektory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fc603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ee04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8416c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(6, 16)\n",
    "embedded_sentence = embed(sentence_int).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be293cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb420c2",
   "metadata": {},
   "source": [
    "# Macierze wag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72df421",
   "metadata": {},
   "source": [
    "Teraz omówmy szeroko stosowany mechanizm samo-uwagi znany jako **scaled dot-product attention**, który stanowi integralny element architektury transformera.\n",
    "\n",
    "Mechanizm samo-uwagi wykorzystuje trzy macierze wag: **W<sub>q</sub>**, **W<sub>k</sub>** oraz **W<sub>v</sub>**, które są dostrajane jako parametry modelu podczas uczenia.  \n",
    "Macierze te służą do rzutowania wektorów wejściowych na odpowiednie reprezentacje: **zapytania (query)**, **klucze (key)** i **wartości (value)**.\n",
    "\n",
    "Odpowiednie sekwencje zapytań, kluczy i wartości otrzymuje się przez mnożenie macierzy wag **W** przez wektory osadzeń wejściowych **x**:\n",
    "\n",
    "- **Sekwencja zapytań:**  \n",
    "  $$\n",
    "  \\mathbf{q}^{(i)} = \\mathbf{W}_q \\mathbf{x}^{(i)} \\quad \\text{dla } i \\in [1, T]\n",
    "  $$\n",
    "\n",
    "- **Sekwencja kluczy:**  \n",
    "  $$\n",
    "  \\mathbf{k}^{(i)} = \\mathbf{W}_k \\mathbf{x}^{(i)} \\quad \\text{dla } i \\in [1, T]\n",
    "  $$\n",
    "\n",
    "- **Sekwencja wartości:**  \n",
    "  $$\n",
    "  \\mathbf{v}^{(i)} = \\mathbf{W}_v \\mathbf{x}^{(i)} \\quad \\text{dla } i \\in [1, T]\n",
    "  $$\n",
    "\n",
    "Indeks *i* odnosi się do pozycji tokena w sekwencji wejściowej, której długość wynosi *T*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b458f2",
   "metadata": {},
   "source": [
    "Zarówno **q<sup>(i)</sup>**, jak i **k<sup>(i)</sup>** są wektorami o wymiarze *dₖ*. Macierze projekcji **W<sub>q</sub>** oraz **W<sub>k</sub>** mają rozmiar $d_k \\times d$, natomiast macierz **W<sub>v</sub>** ma rozmiar $d_v \\times d$. Warto zauważyć, że *d* reprezentuje rozmiar każdego wektora słowa *x*. Ponieważ podczas obliczania iloczynu skalarnego między wektorami zapytań i kluczy wymagane jest, aby te dwa wektory miały tę samą liczbę elementów ($d_q = d_k$), liczba elementów w wektorze wartości **v<sup>(i)</sup>**, która determinuje rozmiar wynikowego wektora kontekstu, może być dowolna. W dalszej części przykładu kodu przyjmujemy więc, że $d_q = d_k = 24$ oraz $d_v = 28$, inicjalizując macierze projekcji w następujący sposób.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539ae045",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6fc381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query: torch.Size([24, 16])\n",
      "W_key:   torch.Size([24, 16])\n",
      "W_value: torch.Size([28, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"W_query:\", W_query.shape)\n",
    "print(\"W_key:  \", W_key.shape)\n",
    "print(\"W_value:\", W_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b99efd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc074e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8aba9",
   "metadata": {},
   "source": [
    "Jak pokazano na ilustracji powyżej, obliczamy $ \\omega_{i,j} $ jako iloczyn skalarny pomiędzy sekwencjami zapytań i kluczy:\n",
    "\n",
    "$$ \\omega_{i,j} = \\mathbf{q}^{(i)^\\top} \\mathbf{k}^{(j)} $$\n",
    "\n",
    "Na przykład możemy obliczyć nieznormalizowaną wagę atencji (attention weight) dla zapytania i piątego elementu wejściowego (odpowiadającego pozycji indeksu 4) w następujący sposób:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a20060c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1466, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b97cc3",
   "metadata": {},
   "source": [
    "Mówi to nam jak bardzo token na pozycji 2 (czyli \"is\") jest powiązany z tokenem na pozycji 4 (czyli \"dessert\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3f4c9",
   "metadata": {},
   "source": [
    "Ponieważ będziemy potrzebować tych wartości do obliczenia wag atencji w dalszej części, obliczmy wartości $ \\omega $ dla wszystkich tokenów wejściowych, tak jak pokazano na poprzedniej ilustracji:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "141ba5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39448c",
   "metadata": {},
   "source": [
    "Skalowanie przez $d_k$ zapewnia, że długość euklidesowa wektorów wag pozostaje w przybliżeniu na tym samym poziomie. Pomaga to zapobiec sytuacji, w której wagi uwagi stają się zbyt małe lub zbyt duże, co mogłoby prowadzić do niestabilności numerycznej lub utrudniać zbieżność modelu podczas uczenia.\n",
    "\n",
    "W kodzie obliczanie wag uwagi można zaimplementować w następujący sposób:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a74933f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd2510",
   "metadata": {},
   "source": [
    "Ostatecznym krokiem jest obliczenie wektora kontekstu $ \\mathbf{z}^{(2)} $, który stanowi wersję naszego oryginalnego wektora zapytania $ \\mathbf{x}^{(2)} $, ważoną przez wagi uwagi. Wektor ten uwzględnia wszystkie pozostałe elementy wejściowe jako kontekst poprzez wagi uwagi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cb6f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8b798",
   "metadata": {},
   "source": [
    "Zauważ, że ten wektor wyjściowy ma więcej wymiarów ($d_v = 28$) niż oryginalny wektor wejściowy ($d = 16$), ponieważ wcześniej przyjęliśmy $d_v > d$. Wybór rozmiaru osadzenia (embeddingu) jest jednak arbitralny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1eb9e",
   "metadata": {},
   "source": [
    "### 🧭 Podsumowanie kroków obliczania samo-uwagi (scaled dot-product attention)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"context-vector.png\" alt=\"Wektor kontekstu\" width=\"600\"/>\n",
    "  <img src=\"single-head.png\" alt=\"Macierze QKV\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "1. **Embedding** – wejściowe słowa (tokeny) zostały zamienione na wektory o ustalonym rozmiarze $d$.  \n",
    "   Każdy token w sekwencji ma więc reprezentację numeryczną $\\mathbf{x}^{(i)} \\in \\mathbb{R}^d$.\n",
    "\n",
    "2. **Projekcje na zapytania, klucze i wartości** – wektory osadzeń $\\mathbf{x}^{(i)}$ zostały przekształcone przy pomocy trzech macierzy wag:  \n",
    "   $$\n",
    "   \\mathbf{q}^{(i)} = \\mathbf{W}_q \\mathbf{x}^{(i)}, \\quad\n",
    "   \\mathbf{k}^{(i)} = \\mathbf{W}_k \\mathbf{x}^{(i)}, \\quad\n",
    "   \\mathbf{v}^{(i)} = \\mathbf{W}_v \\mathbf{x}^{(i)}\n",
    "   $$\n",
    "   gdzie $\\mathbf{W}_q$, $\\mathbf{W}_k$, $\\mathbf{W}_v$ mają odpowiednio wymiary $(d_q \\times d)$, $(d_k \\times d)$ i $(d_v \\times d)$.\n",
    "\n",
    "3. **Obliczenie surowych wag uwagi** – dla każdej pary tokenów $(i, j)$ obliczono współczynnik podobieństwa (ang. *attention score*) jako iloczyn skalarny:  \n",
    "   $$\n",
    "   \\omega_{i,j} = \\mathbf{q}^{(i)^\\top} \\mathbf{k}^{(j)}\n",
    "   $$\n",
    "\n",
    "4. **Skalowanie przez $d_k$** – wartości $\\omega_{i,j}$ zostały przeskalowane przez $\\sqrt{d_k}$, aby utrzymać stabilną wielkość gradientów i zapobiec zbyt dużym lub małym wartościom uwagi.\n",
    "\n",
    "5. **Normalizacja (softmax)** – przeskalowane wagi zostały znormalizowane funkcją softmax, tak aby ich suma dla każdego zapytania wynosiła 1 (interpretacja jako rozkład prawdopodobieństwa).\n",
    "\n",
    "6. **Obliczenie wektora kontekstu** – dla każdego tokena $i$ obliczono wektor kontekstu jako średnią ważoną wektorów wartości $\\mathbf{v}^{(j)}$ z wagami uwagi:  \n",
    "   $$\n",
    "   \\mathbf{z}^{(i)} = \\sum_j \\text{softmax}(\\omega_{i,j}) \\, \\mathbf{v}^{(j)}\n",
    "   $$\n",
    "\n",
    "7. **Interpretacja** – otrzymany wektor $\\mathbf{z}^{(i)}$ jest nową reprezentacją tokena, która uwzględnia jego kontekst w całej sekwencji (czyli „na co token zwraca uwagę”).\n",
    "\n",
    "nasze przykładowe zdanie:  \n",
    "**\"Life is short, eat dessert first\"**\n",
    "\n",
    "Słownik tokenów:  \n",
    "`{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}`\n",
    "\n",
    "| Symbol | Znaczenie | Typ danych | Rola | Przykład (dla zdania *Life is short, eat dessert first*) |\n",
    "|:--|:--|:--|:--|:--|\n",
    "| **$\\mathbf{x}^{(i)}$** | wektor osadzenia (*embedding*) tokena wejściowego | wektor $\\in \\mathbb{R}^d$ | „Jak wygląda moje słowo w przestrzeni cech?” | wektor reprezentujący znaczenie słowa **Life** w przestrzeni embeddingów |\n",
    "| **$\\mathbf{q}^{(i)}$** | wektor zapytania (*query*) | wektor $\\in \\mathbb{R}^{d_q}$ | „Czego szukam?” | wektor określający, czego **Life** szuka w innych słowach zdania |\n",
    "| **$\\mathbf{k}^{(j)}$** | wektor klucza (*key*) | wektor $\\in \\mathbb{R}^{d_k}$ | „Jakie mam cechy?” | wektor opisujący cechy słowa **dessert**, które mogą przyciągnąć uwagę |\n",
    "| **$\\mathbf{v}^{(j)}$** | wektor wartości (*value*) | wektor $\\in \\mathbb{R}^{d_v}$ | „Jaką niosę informację?” | reprezentacja semantyczna słowa **dessert**, która może być przekazana dalej |\n",
    "| **$\\omega_{i,j}$** | surowy wynik podobieństwa | skalar | „Na ile podobni jesteśmy?” | wartość określająca podobieństwo między **Life** a **dessert** |\n",
    "| **$\\alpha_{i,j}$** | waga uwagi (po softmaxie) | skalar | „Jak bardzo się liczę?” | waga określająca, ile uwagi **Life** poświęca słowu **dessert** |\n",
    "| **$\\mathbf{z}^{(i)}$** | wektor kontekstu | wektor $\\in \\mathbb{R}^{d_v}$ | „Nowa reprezentacja tokena *i*” | nowa reprezentacja **Life**, uwzględniająca kontekst całego zdania |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49977ce",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760fc15",
   "metadata": {},
   "source": [
    "Jak sama nazwa wskazuje, **multi-head attention** (uwaga wielogłowa) obejmuje wiele takich głów — każda z nich składa się z własnych macierzy **zapytania (query)**, **klucza (key)** oraz **wartości (value)**.  \n",
    "Koncepcja ta jest analogiczna do wykorzystania wielu jąder (*kernels*) w konwolucyjnych sieciach neuronowych (CNN).\n",
    "\n",
    "Aby zilustrować to w kodzie, załóżmy, że mamy **3 głowy uwagi**.  \n",
    "W takim przypadku rozszerzamy macierze wag z wymiaru $d' \\times d$ do $3 \\times d' \\times d$, tak aby każda głowa miała swój własny zestaw wag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f83f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "727eece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eccb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b74c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 6])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08947f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cc8b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf003f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
