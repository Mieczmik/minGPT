{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "source": [
        "# Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tokenizers\n",
        "import unicodedata\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_tokenizer = tokenizers.Tokenizer.from_file(\"en_tokenizer.json\")\n",
        "pl_tokenizer = tokenizers.Tokenizer.from_file(\"pl_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize text\n",
        "# each line of the file is in the format \"<english>\\t<french>\"\n",
        "# We convert text to lowercase, normalize unicode (UFKC)\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) < 2:\n",
        "        return None  # pomiń niepoprawne linie\n",
        "\n",
        "    # niektóre linie mają więcej niż jedno tłumaczenie – weź tylko pierwsze dwa\n",
        "    eng, pl = parts[0], parts[1]\n",
        "\n",
        "    return eng.strip(), pl.strip()\n",
        "\n",
        "text_pairs = []\n",
        "with zipfile.ZipFile(\"pol-eng.zip\", \"r\") as zip_ref:\n",
        "    for line in zip_ref.read(\"pol.txt\").decode(\"utf-8\").splitlines():\n",
        "        eng, pol = normalize(line)\n",
        "        text_pairs.append((eng, pol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x70e61ff56db0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ================= CHANGED: hyperparamy =================\n",
        "batch_size   = 1024\n",
        "block_size   = 128      # musi >= max długość sekwencji w batchu\n",
        "max_epochs   = 200\n",
        "learning_rate= 1e-3\n",
        "device       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd       = 64\n",
        "n_head       = 1\n",
        "n_layer      = 4\n",
        "dropout      = 0.0\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "tok = pl_tokenizer\n",
        "vocab_size = tok.get_vocab_size()\n",
        "pad_id = tok.token_to_id(\"[pad]\")\n",
        "bos_id = tok.token_to_id(\"[start]\")\n",
        "eos_id = tok.token_to_id(\"[end]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LMTranslationDataset(Dataset):\n",
        "    def __init__(self, text_pairs):\n",
        "        self.text_pairs = text_pairs\n",
        "    def __len__(self):\n",
        "        return len(self.text_pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        eng, pol = self.text_pairs[idx]\n",
        "        # pełna sekwencja do teacher forcing\n",
        "        s = f\"[start] EN: {eng}\\nPL: {pol} [end]\"\n",
        "        return s\n",
        "\n",
        "def collate_lm(batch):\n",
        "    enc = tok.encode_batch(batch, add_special_tokens=True)  # jeśli post-processor dodaje BOS/EOS, OK\n",
        "    ids = [e.ids[:block_size] for e in enc]                # przytnij do block_size\n",
        "    # padding zapewnia tok.enable_padding; jeśli nie masz, zrób ręcznie\n",
        "    x = torch.tensor([seq[:-1] for seq in ids], dtype=torch.long)  # input\n",
        "    y = torch.tensor([seq[1:]  for seq in ids], dtype=torch.long)  # target (shift)\n",
        "    return x, y\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    LMTranslationDataset(text_pairs),\n",
        "    batch_size=batch_size, shuffle=True,\n",
        "    collate_fn=collate_lm\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 00 | loss 4.9913\n",
            "epoch 01 | loss 4.1280\n",
            "epoch 02 | loss 3.6581\n",
            "epoch 03 | loss 3.4677\n",
            "epoch 04 | loss 3.2362\n",
            "epoch 05 | loss 3.1254\n",
            "epoch 06 | loss 3.0177\n",
            "epoch 07 | loss 2.9145\n",
            "epoch 08 | loss 2.8476\n",
            "epoch 09 | loss 2.7419\n",
            "epoch 10 | loss 2.6730\n",
            "epoch 11 | loss 2.6608\n",
            "epoch 12 | loss 2.6457\n",
            "epoch 13 | loss 2.5637\n",
            "epoch 14 | loss 2.5304\n",
            "epoch 15 | loss 2.4656\n",
            "epoch 16 | loss 2.4263\n",
            "epoch 17 | loss 2.4674\n",
            "epoch 18 | loss 2.4014\n",
            "epoch 19 | loss 2.3288\n",
            "epoch 20 | loss 2.2874\n",
            "epoch 21 | loss 2.2932\n",
            "epoch 22 | loss 2.3121\n",
            "epoch 23 | loss 2.2398\n",
            "epoch 24 | loss 2.1978\n",
            "epoch 25 | loss 2.2393\n",
            "epoch 26 | loss 2.2093\n",
            "epoch 27 | loss 2.1422\n",
            "epoch 28 | loss 2.1578\n",
            "epoch 29 | loss 2.0760\n",
            "epoch 30 | loss 2.1332\n",
            "epoch 31 | loss 2.0499\n",
            "epoch 32 | loss 2.0916\n",
            "epoch 33 | loss 2.1133\n",
            "epoch 34 | loss 1.9859\n",
            "epoch 35 | loss 2.0571\n",
            "epoch 36 | loss 1.9836\n",
            "epoch 37 | loss 2.0296\n",
            "epoch 38 | loss 1.9672\n",
            "epoch 39 | loss 1.9733\n",
            "epoch 40 | loss 1.9870\n",
            "epoch 41 | loss 1.9876\n",
            "epoch 42 | loss 1.9274\n",
            "epoch 43 | loss 1.9324\n",
            "epoch 44 | loss 1.9240\n",
            "epoch 45 | loss 1.9097\n",
            "epoch 46 | loss 1.9024\n",
            "epoch 47 | loss 1.8994\n",
            "epoch 48 | loss 1.8937\n",
            "epoch 49 | loss 1.8667\n",
            "epoch 50 | loss 1.8495\n",
            "epoch 51 | loss 1.8639\n",
            "epoch 52 | loss 1.8894\n",
            "epoch 53 | loss 1.8378\n",
            "epoch 54 | loss 1.8139\n",
            "epoch 55 | loss 1.8357\n",
            "epoch 56 | loss 1.8126\n",
            "epoch 57 | loss 1.8083\n",
            "epoch 58 | loss 1.8167\n",
            "epoch 59 | loss 1.8256\n",
            "epoch 60 | loss 1.7666\n",
            "epoch 61 | loss 1.8478\n",
            "epoch 62 | loss 1.8428\n",
            "epoch 63 | loss 1.7930\n",
            "epoch 64 | loss 1.7831\n",
            "epoch 65 | loss 1.8066\n",
            "epoch 66 | loss 1.7558\n",
            "epoch 67 | loss 1.7643\n",
            "epoch 68 | loss 1.7517\n",
            "epoch 69 | loss 1.7591\n",
            "epoch 70 | loss 1.7459\n",
            "epoch 71 | loss 1.6931\n",
            "epoch 72 | loss 1.7341\n",
            "epoch 73 | loss 1.7572\n",
            "epoch 74 | loss 1.6975\n",
            "epoch 75 | loss 1.7399\n",
            "epoch 76 | loss 1.7611\n",
            "epoch 77 | loss 1.7478\n",
            "epoch 78 | loss 1.6640\n",
            "epoch 79 | loss 1.7093\n",
            "epoch 80 | loss 1.6661\n",
            "epoch 81 | loss 1.6674\n",
            "epoch 82 | loss 1.6695\n",
            "epoch 83 | loss 1.6721\n",
            "epoch 84 | loss 1.6972\n",
            "epoch 85 | loss 1.6511\n",
            "epoch 86 | loss 1.6609\n",
            "epoch 87 | loss 1.6838\n",
            "epoch 88 | loss 1.6480\n",
            "epoch 89 | loss 1.6848\n",
            "epoch 90 | loss 1.6622\n",
            "epoch 91 | loss 1.6174\n",
            "epoch 92 | loss 1.6359\n",
            "epoch 93 | loss 1.6369\n",
            "epoch 94 | loss 1.6171\n",
            "epoch 95 | loss 1.6572\n",
            "epoch 96 | loss 1.6424\n",
            "epoch 97 | loss 1.5733\n",
            "epoch 98 | loss 1.6255\n",
            "epoch 99 | loss 1.5996\n",
            "epoch 100 | loss 1.5931\n",
            "epoch 101 | loss 1.6338\n",
            "epoch 102 | loss 1.6201\n",
            "epoch 103 | loss 1.6164\n",
            "epoch 104 | loss 1.6269\n",
            "epoch 105 | loss 1.6279\n",
            "epoch 106 | loss 1.6003\n",
            "epoch 107 | loss 1.5865\n",
            "epoch 108 | loss 1.6161\n",
            "epoch 109 | loss 1.6164\n",
            "epoch 110 | loss 1.5678\n",
            "epoch 111 | loss 1.6094\n",
            "epoch 112 | loss 1.5999\n",
            "epoch 113 | loss 1.6182\n",
            "epoch 114 | loss 1.6249\n",
            "epoch 115 | loss 1.5758\n",
            "epoch 116 | loss 1.6201\n",
            "epoch 117 | loss 1.5761\n",
            "epoch 118 | loss 1.5732\n",
            "epoch 119 | loss 1.5857\n",
            "epoch 120 | loss 1.5939\n",
            "epoch 121 | loss 1.5665\n",
            "epoch 122 | loss 1.5653\n",
            "epoch 123 | loss 1.5620\n",
            "epoch 124 | loss 1.5372\n",
            "epoch 125 | loss 1.5360\n",
            "epoch 126 | loss 1.5146\n",
            "epoch 127 | loss 1.5423\n",
            "epoch 128 | loss 1.5174\n",
            "epoch 129 | loss 1.5551\n",
            "epoch 130 | loss 1.5172\n",
            "epoch 131 | loss 1.5372\n",
            "epoch 132 | loss 1.5158\n",
            "epoch 133 | loss 1.5539\n",
            "epoch 134 | loss 1.5163\n",
            "epoch 135 | loss 1.5323\n",
            "epoch 136 | loss 1.5522\n",
            "epoch 137 | loss 1.5190\n",
            "epoch 138 | loss 1.5071\n",
            "epoch 139 | loss 1.5131\n",
            "epoch 140 | loss 1.5448\n",
            "epoch 141 | loss 1.4803\n",
            "epoch 142 | loss 1.5389\n",
            "epoch 143 | loss 1.5628\n",
            "epoch 144 | loss 1.5441\n",
            "epoch 145 | loss 1.5185\n",
            "epoch 146 | loss 1.5271\n",
            "epoch 147 | loss 1.4815\n",
            "epoch 148 | loss 1.5261\n",
            "epoch 149 | loss 1.5180\n",
            "epoch 150 | loss 1.4943\n",
            "epoch 151 | loss 1.5247\n",
            "epoch 152 | loss 1.5402\n",
            "epoch 153 | loss 1.5351\n",
            "epoch 154 | loss 1.5197\n",
            "epoch 155 | loss 1.5319\n",
            "epoch 156 | loss 1.5348\n",
            "epoch 157 | loss 1.5126\n",
            "epoch 158 | loss 1.4922\n",
            "epoch 159 | loss 1.5044\n",
            "epoch 160 | loss 1.4565\n",
            "epoch 161 | loss 1.5313\n",
            "epoch 162 | loss 1.5251\n",
            "epoch 163 | loss 1.4850\n",
            "epoch 164 | loss 1.5303\n",
            "epoch 165 | loss 1.5315\n",
            "epoch 166 | loss 1.4605\n",
            "epoch 167 | loss 1.4457\n",
            "epoch 168 | loss 1.4415\n",
            "epoch 169 | loss 1.5315\n",
            "epoch 170 | loss 1.4874\n",
            "epoch 171 | loss 1.5091\n",
            "epoch 172 | loss 1.4425\n",
            "epoch 173 | loss 1.4461\n",
            "epoch 174 | loss 1.4678\n",
            "epoch 175 | loss 1.4611\n",
            "epoch 176 | loss 1.4761\n",
            "epoch 177 | loss 1.4646\n",
            "epoch 178 | loss 1.4379\n",
            "epoch 179 | loss 1.4867\n",
            "epoch 180 | loss 1.4727\n",
            "epoch 181 | loss 1.4437\n",
            "epoch 182 | loss 1.4366\n",
            "epoch 183 | loss 1.4901\n",
            "epoch 184 | loss 1.4400\n",
            "epoch 185 | loss 1.4559\n",
            "epoch 186 | loss 1.4343\n",
            "epoch 187 | loss 1.4448\n",
            "epoch 188 | loss 1.4515\n",
            "epoch 189 | loss 1.4166\n",
            "epoch 190 | loss 1.4752\n",
            "epoch 191 | loss 1.4313\n",
            "epoch 192 | loss 1.4503\n",
            "epoch 193 | loss 1.4206\n",
            "epoch 194 | loss 1.4359\n",
            "epoch 195 | loss 1.4192\n",
            "epoch 196 | loss 1.4607\n",
            "epoch 197 | loss 1.4649\n",
            "epoch 198 | loss 1.4303\n",
            "epoch 199 | loss 1.4793\n",
            " : run!: uciekaj! \n"
          ]
        }
      ],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class DecoderOnlyLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, padding_idx=pad_id)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        # (opcjonalnie) powiąż wagi head z embeddingiem: self.lm_head.weight = self.token_embedding_table.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        pos = torch.arange(T, device=idx.device)  # [T]\n",
        "        tok_emb = self.token_embedding_table(idx)           # [B,T,C]\n",
        "        pos_emb = self.position_embedding_table(pos)[None]  # [1,T,C]\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                            # [B,T,V]\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
        "                                   targets.view(-1),\n",
        "                                   ignore_index=pad_id)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=64, temperature=1.0, top_k=None, stop_id=None):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "            if stop_id is not None and next_id.item() == stop_id:\n",
        "                break\n",
        "        return idx\n",
        "\n",
        "model = DecoderOnlyLM().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ================= CHANGED: trening na DataLoaderze =================\n",
        "for epoch in range(max_epochs):\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        _, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    print(f\"epoch {epoch:02d} | loss {loss.item():.4f}\")\n",
        "\n",
        "# ================= CHANGED: generowanie tłumaczenia =================\n",
        "@torch.no_grad()\n",
        "def translate(eng_sentence: str, max_new_tokens=64):\n",
        "    prefix = f\"[start] EN: {eng_sentence}\\nPL:\"\n",
        "    ids = tok.encode(prefix, add_special_tokens=True).ids\n",
        "    idx = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "    out = model.generate(idx, max_new_tokens=max_new_tokens, temperature=0.8, top_k=50, stop_id=eos_id)\n",
        "    text = tok.decode(out[0].tolist())\n",
        "    # (opcjonalnie) wytnij fragment po \"PL:\" i przed [end]\n",
        "    return text\n",
        "\n",
        "print( translate(\"run!\") )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' : hello! what is your name?: cześć, co jest twoje? '"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"hello! what is your name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
